# -*- coding: utf-8 -*-
"""image-2-image using diffusers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb

# Image2Image Pipeline for Stable Diffusion using ðŸ§¨ Diffusers 

This notebook shows how to create a custom `diffusers` pipeline for  text-guided image-to-image generation with Stable Diffusion model using  ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers). 

For a general introduction to the Stable Diffusion model please refer to this [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).
"""

# !nvidia-smi

# !pip install diffusers==0.3.0 transformers ftfy
# !pip install -qq "ipywidgets>=7,<8"

"""You need to accept the model license before downloading or using the weights. In this post we'll use model version `v1-4`, so you'll need to  visit [its card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. 

You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).
"""

# from huggingface_hub import notebook_login
#
# notebook_login()

"""## Image2Image pipeline."""

import inspect
import warnings
from typing import List, Optional, Union

import torch
from torch import autocast
from tqdm.auto import tqdm
import matplotlib
matplotlib.use('TkAgg')
# from remote_plot import plt
import matplotlib.pyplot as plt


# from diffusers import StableDiffusionImg2ImgPipeline
from diffusers import StableDiffusionDepth2ImgPipeline

# import pydevd_pycharm
# pydevd_pycharm.settrace('localhost', port=12345, stdoutToServer=True,
#                         stderrToServer=True)


"""Load the pipeline"""

device = "cuda:2"
model_path = "stabilityai/stable-diffusion-2-depth"

pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(
    model_path,
    revision="fp16", 
    torch_dtype=torch.float16,
    use_auth_token=True
)
pipe = pipe.to(device)

"""Download an initial image and preprocess it so we can pass it to the pipeline."""

import requests
from io import BytesIO
from PIL import Image

# url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
url = "http://images.cocodataset.org/val2017/000000039769.jpg"

# response = requests.get(url)
# init_img = Image.open(BytesIO(response.content)).convert("RGB")
# init_img = init_img.resize((768, 512))
# init_img

init_img = Image.open(requests.get(url, stream=True).raw)
"""Define the prompt and run the pipeline."""

# prompt = "A fantasy landscape, trending on artstation"
prompt = "two tigers"
n_prompt = "bad, deformed, ugly, bad anotomy"

"""Here, `strength` is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input."""

generator = torch.Generator(device=device).manual_seed(1024)
with autocast("cuda"):
    image = pipe(prompt=prompt, image=init_img, negative_prompt=n_prompt, strength=0.75, guidance_scale=7.5, generator=generator).images[0]

# image

with autocast("cuda"):
    image = pipe(prompt=prompt, image=init_img, negative_prompt=n_prompt, strength=0.5, guidance_scale=7.5, generator=generator).images[0]

# image

"""As you can see, when using a lower value for `strength`, the generated image is more closer to the original `init_image`

Now using [LMSDiscreteScheduler](https://huggingface.co/docs/diffusers/api/schedulers#diffusers.LMSDiscreteScheduler)
"""

from diffusers import LMSDiscreteScheduler

lms = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
pipe.scheduler = lms

generator = torch.Generator(device=device).manual_seed(1024)
with autocast("cuda"):
    image = pipe(prompt=prompt, image=init_img, negative_prompt=n_prompt, strength=0.75, guidance_scale=7.5, generator=generator).images[0]

image